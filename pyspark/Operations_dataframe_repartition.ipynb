{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1687d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import findspark\n",
    "findspark.init('D:/Software/Study/Spark/spark-3.1.3-bin-hadoop3.2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD\n",
    "from sedona.core.SpatialRDD import PointRDD\n",
    "from sedona.core.SpatialRDD import CircleRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a41bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KryoSerializer.getName and SedonaKryoRegistrator.getName class properties to reduce memory impact.\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Sedona App\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.1.0-incubating,org.datasyslab:geotools-wrapper:1.1.0-25.2\") .\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0d28c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register function is essential for Apache Sedona Core and Apache Sedona SQL. \n",
    "# this command will enable you to call the provided APIs\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b063b8",
   "metadata": {},
   "source": [
    "Read the first PointRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d3a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of point: 7\n",
      "time consumed to create a point rdd: 0.4697861671447754\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "input_location = \"D:/Programming/Python/GEOG770/Project/data/input_vertices_1_pt.csv\"\n",
    "offset = 0  # The point starts from Column 0\n",
    "splitter = FileDataSplitter.CSV # FileDataSplitter enumeration, this parameter will pass the csv format to SpatialRDD\n",
    "carry_other_attributes = True  # Carry othrt attributes, Column 2 (hotel, gas, bar...)\n",
    "level = StorageLevel.MEMORY_ONLY # Storage level from pyspark, this parameter will tell Spark to cache the \"rawSpatialRDD\"\n",
    "\n",
    "# create point_rdd\n",
    "point_rdd = PointRDD(sc, input_location, offset, splitter, carry_other_attributes, StorageLevel.MEMORY_ONLY)\n",
    "point_rdd.analyze()\n",
    "\n",
    "print(\"number of point:\", point_rdd.approximateTotalCount)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"time consumed to create a point rdd:\", t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadedf1",
   "metadata": {},
   "source": [
    "Read the second PointRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5b79b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of point: 15\n"
     ]
    }
   ],
   "source": [
    "input_location_2 = \"D:/Programming/Python/GEOG770/Project/data/input_vertices_2_pt.csv\"\n",
    "\n",
    "point_rdd_2 = PointRDD(sc, input_location_2, offset, splitter, carry_other_attributes, StorageLevel.MEMORY_ONLY)\n",
    "point_rdd_2.analyze()\n",
    "\n",
    "print(\"number of point:\", point_rdd_2.approximateTotalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85fa0ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        #StructField(\"point\", GeometryType(), True),\n",
    "        StructField(\"x\", DoubleType(), True),\n",
    "        StructField(\"y\", DoubleType(), True),\n",
    "        StructField(\"ele\", DoubleType(), True),\n",
    "        # the third boolean variable means whether the field can be null (None) or not\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1368743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1 = point_rdd.rawSpatialRDD.map(lambda x: [x.geom.x, x.geom.y, float(x.userData)])\n",
    "df_pt1 = spark.createDataFrame(pt1, schema, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "296dc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|    x|    y| ele|\n",
      "+-----+-----+----+\n",
      "| 40.0| 20.0| 4.0|\n",
      "| 22.0|400.0| 2.0|\n",
      "| 66.0|200.0| 8.0|\n",
      "|260.0| 80.0| 1.0|\n",
      "|200.0|200.0|10.0|\n",
      "+-----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dad942c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt2 = point_rdd_2.rawSpatialRDD.map(lambda x: [x.geom.x, x.geom.y, float(x.userData)])\n",
    "df_pt2 = spark.createDataFrame(pt2, schema, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9c4eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|    x|    y| ele|\n",
      "+-----+-----+----+\n",
      "| 40.0| 20.0|12.0|\n",
      "| 50.0|400.0|23.0|\n",
      "| 66.0|200.0|20.0|\n",
      "|260.0|100.0|18.0|\n",
      "|200.0|200.0|10.0|\n",
      "+-----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt2.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c47d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "schema_geo = StructType(\n",
    "    [\n",
    "        StructField(\"point\", GeometryType(), True),\n",
    "        #StructField(\"x\", DoubleType(), True),\n",
    "        #StructField(\"y\", DoubleType(), True),\n",
    "        StructField(\"ele\", DoubleType(), True),\n",
    "        # the third boolean variable means whether the field can be null (None) or not\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a10e59",
   "metadata": {},
   "source": [
    "Create a DataFrame from a PointRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345a712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1_geo = point_rdd.rawSpatialRDD.map(lambda x: [x.geom, float(x.userData)])\n",
    "df_pt1_geo = spark.createDataFrame(pt1_geo, schema_geo, verifySchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4cdd25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo_col = df_pt1_geo.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ff65c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shapely.geometry.point.Point"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pt1_geo_col[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54a9d3",
   "metadata": {},
   "source": [
    "pt1.distance(pt2) to calculate the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "615a980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380.4260769190251"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt1_geo_col[0][0].distance(df_pt1_geo_col[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e5e6ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point: geometry (nullable = true)\n",
      " |-- ele: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9986dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, log, lit, first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad2526",
   "metadata": {},
   "source": [
    "add a column to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac967d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.withColumn(\"disRaw2Max\", lit(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72237440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo_col = df_pt1_geo.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a54b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----------+\n",
      "|          point| ele|disRaw2Max|\n",
      "+---------------+----+----------+\n",
      "|  POINT (40 20)| 4.0|       0.1|\n",
      "| POINT (22 400)| 2.0|       0.1|\n",
      "| POINT (66 200)| 8.0|       0.1|\n",
      "| POINT (260 80)| 1.0|       0.1|\n",
      "|POINT (200 200)|10.0|       0.1|\n",
      "+---------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d4918",
   "metadata": {},
   "source": [
    "using a UDF when adding a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf163e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def get_dis(pt1,pt2):\n",
    "    #dis_temp = x.distance(y)\n",
    "    dis_temp = pt1.x + pt2.x\n",
    "    return dis_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f84ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.withColumn(\"disRaw2Max\", lit(get_dis(df_pt1_geo.point, df_pt1_geo.point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdec6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo_col = df_pt1_geo.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f3ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----------+\n",
      "|          point| ele|disRaw2Max|\n",
      "+---------------+----+----------+\n",
      "|  POINT (40 20)| 4.0|      80.0|\n",
      "| POINT (22 400)| 2.0|      44.0|\n",
      "| POINT (66 200)| 8.0|     132.0|\n",
      "| POINT (260 80)| 1.0|     520.0|\n",
      "|POINT (200 200)|10.0|     400.0|\n",
      "|POINT (300 300)| 5.0|     600.0|\n",
      "|POINT (400 400)|20.0|     800.0|\n",
      "+---------------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2b595",
   "metadata": {},
   "source": [
    "Convert a Point geometry to double columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa34673",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def get_x(pt1):\n",
    "    return pt1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "124a269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def get_y(pt1):\n",
    "    return pt1.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7f98a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.withColumn(\"raw_x\",lit(get_x(df_pt1_geo.point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0c87c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.withColumn(\"raw_y\",lit(get_y(df_pt1_geo.point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02a01661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point: geometry (nullable = true)\n",
      " |-- ele: double (nullable = true)\n",
      " |-- disRaw2Max: string (nullable = true)\n",
      " |-- raw_x: string (nullable = true)\n",
      " |-- raw_y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d48e03",
   "metadata": {},
   "source": [
    "reorder the order of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25eb7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.select(\"point\",\"raw_x\",\"raw_y\",\"ele\",\"disRaw2Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5c6399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- point: geometry (nullable = true)\n",
      " |-- raw_x: string (nullable = true)\n",
      " |-- raw_y: string (nullable = true)\n",
      " |-- ele: double (nullable = true)\n",
      " |-- disRaw2Max: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091bd9c",
   "metadata": {},
   "source": [
    "drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2c8bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.drop(\"point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e28e924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----------+\n",
      "|raw_x|raw_y| ele|disRaw2Max|\n",
      "+-----+-----+----+----------+\n",
      "| 40.0| 20.0| 4.0|      80.0|\n",
      "| 22.0|400.0| 2.0|      44.0|\n",
      "| 66.0|200.0| 8.0|     132.0|\n",
      "|260.0| 80.0| 1.0|     520.0|\n",
      "|200.0|200.0|10.0|     400.0|\n",
      "|300.0|300.0| 5.0|     600.0|\n",
      "|400.0|400.0|20.0|     800.0|\n",
      "+-----+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46415245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo = df_pt1_geo.withColumn(\"radius\", lit(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82d31391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+----------+------+\n",
      "|raw_x|raw_y| ele|disRaw2Max|radius|\n",
      "+-----+-----+----+----------+------+\n",
      "| 40.0| 20.0| 4.0|      80.0|   2.0|\n",
      "| 22.0|400.0| 2.0|      44.0|   2.0|\n",
      "| 66.0|200.0| 8.0|     132.0|   2.0|\n",
      "|260.0| 80.0| 1.0|     520.0|   2.0|\n",
      "|200.0|200.0|10.0|     400.0|   2.0|\n",
      "|300.0|300.0| 5.0|     600.0|   2.0|\n",
      "|400.0|400.0|20.0|     800.0|   2.0|\n",
      "+-----+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "818d01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='D:/Programming/Python/GEOG770/Project/data/folder_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d3621",
   "metadata": {},
   "source": [
    "output each partition to a same folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79674ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo.write.partitionBy(\"disRaw2Max\",\"radius\").mode(\"append\").csv(\"D:/Programming/Python/GEOG770/Project/data/folder_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c232c",
   "metadata": {},
   "source": [
    "using repartition to make the same group in one file, otherwise it may be in multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cb74226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo.repartition(\"disRaw2Max\",\"radius\").write.partitionBy(\"disRaw2Max\",\"radius\").mode(\"append\").csv(\"D:/Programming/Python/GEOG770/Project/data/folder_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a275b",
   "metadata": {},
   "source": [
    "coalesce(n) will assign n computer cores to output the data, if n is 1, there is no parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a275303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt1_geo.coalesce(3).write.partitionBy(\"disRaw2Max\",\"radius\").mode(\"append\").csv(\"D:/Programming/Python/GEOG770/Project/data/folder_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12cd5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_x: string (nullable = true)\n",
      " |-- raw_y: string (nullable = true)\n",
      " |-- ele: double (nullable = true)\n",
      " |-- disRaw2Max: string (nullable = true)\n",
      " |-- radius: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pt1_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d30dca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_pt1_geo.select('raw_x','raw_y','ele','disRaw2Max','radius')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5999c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_x: string (nullable = true)\n",
      " |-- raw_y: string (nullable = true)\n",
      " |-- ele: double (nullable = true)\n",
      " |-- disRaw2Max: string (nullable = true)\n",
      " |-- radius: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f23b6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def MergeAsString(pt_x,pt_y):\n",
    "    str_temp = str(pt_x)+','+str(pt_y)\n",
    "    #str_temp = str(pt_x)+str(pt_y)\n",
    "    return str_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75a049cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn(\"raw_xy\", lit(MergeAsString(test.raw_x, test.raw_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78cb1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raw_x: string (nullable = true)\n",
      " |-- raw_y: string (nullable = true)\n",
      " |-- ele: double (nullable = true)\n",
      " |-- disRaw2Max: string (nullable = true)\n",
      " |-- radius: double (nullable = false)\n",
      " |-- raw_xy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c7f6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(\"raw_x\")\n",
    "test = test.drop(\"raw_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "77b4e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ele: double (nullable = true)\n",
      " |-- disRaw2Max: string (nullable = true)\n",
      " |-- radius: double (nullable = false)\n",
      " |-- raw_xy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ae82d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.repartition(\"raw_xy\").write.partitionBy(\"raw_xy\").mode(\"append\").csv(\"D:/Programming/Python/GEOG770/Project/data/folder_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a982b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.repartition(\"ele\").write.partitionBy(\"ele\").mode(\"append\").csv(\"D:/Programming/Python/GEOG770/Project/data/folder_8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
